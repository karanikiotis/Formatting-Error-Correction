{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Functions - Classes used from NTLK Package\n",
    "\n",
    "#1. ngrams: used to make N-grams of a specified order\n",
    "\n",
    "#2. pad_both_ends: used to add start & end symbol to our corpus files-sentences. Order should be chosen according to n-grams order\n",
    "\n",
    "#3. everygrams: used to make n-grams of all orders. The highest order is defined through max_len parameter\n",
    "\n",
    "#4. flatten: Creates the vocab of our ngrams model. It consists of all words-tokens of our corpus\n",
    "\n",
    "#5. padded_everygram_pipeline: creates two iterators:\n",
    "        #i. sentences padded and turned into sequences \n",
    "        #ii. sentences padded as above and chained together for a flat stream of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'C:\\CodeRepository\\Formatting-Error-Correction')\n",
    "from datetime import datetime\n",
    "\n",
    "from Scripts.tokenizer import tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline,pad_both_ends\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.util import everygrams, ngrams\n",
    "from Scripts.S1_corpus_bigrams_occurences import count_occur\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<com>', '<eos>', '<com>', '<eos>', '<keyword>', '<space>', '<word>', '<dot>', '<word>', '<dot>', '<word>', '<dot>', '<word>', '<semicolon>', '<eos>', '<eos>', '<keyword>', '<space>', '<keyword>', '<space>', '<word>', '<space>', '<keyword>', '<space>', '<word>', '<eos>', '<open_curly>', '<eos>', '<spacetab>', '<keyword>', '<space>', '<word>', '<open_par>', '<word>', '<space>', '<word>', '<comma>', '<space>', '<word>', '<space>', '<word>', '<close_par>', '<eos>', '<spacetab>', '<open_curly>', '<eos>', '<spacetab>', '<spacetab>', '<keyword>', '<open_par>', '<space>', '<word>', '<dot>', '<word>', '<close_par>', '<semicolon>', '<eos>', '<eos>', '<spacetab>', '<spacetab>', '<word>', '<open_par>', '<word>', '<dot>', '<word>', '<dot>', '<word>', '<open_par>', '<word>', '<comma>', '<space>', '<word>', '<close_par>', '<close_par>', '<semicolon>', '<eos>', '<spacetab>', '<close_curly>', '<eos>', '<eos>', '<spacetab>', '<keyword>', '<space>', '<word>', '<open_par>', '<lit>', '<space>', '<word>', '<close_par>', '<eos>', '<spacetab>', '<open_curly>', '<eos>', '<spacetab>', '<spacetab>', '<keyword>', '<open_par>', '<word>', '<close_par>', '<semicolon>', '<eos>', '<spacetab>', '<close_curly>', '<eos>', '<eos>', '<close_curly>', '<eos>', '<com>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "tokens_available = [\"<start>\",\"<end>\",\"<eos>\",\"<tab>\",\"<spacetab>\",\"<dot>\",\"<comma>\",\"<semicolon>\",\"<colon>\",\"<exclamation>\",\n",
    "                    \"<at>\",\"<hash>\",\"<dollar>\",\"<perc>\",\"<caret>\",\"<and>\",\"<power>\",\"<open_par>\",\"<close_par>\",\"<minus>\",\n",
    "                    \"<plus>\",\"<equal>\",\"<open_bracket>\",\"<close_bracket>\",\"<open_curly>\",\"<close_curly>\",\"<less_than>\",\n",
    "                    \"<greater_than>\",\"<quest>\",\"<back_slash>\",\"<keyword>\",\"<literal>\",\"<lit>\",\"<word>\",\"<space>\",\n",
    "                    \"<com>\",\"<slash>\",\"<string>\",\"<char>\",\"<number>\",\"<unk>\"]\n",
    "\n",
    "replacements = [\"\",\"\",\"\\n\",\"\\t\",\"    \",\".\",\",\",\";\",\":\",\"!\",\"@\",\"#\",\"$\", \"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"-\",\"+\",\"=\",\"[\",\"]\",\"{\",\n",
    "               \"}\",\"<\",\">\",\"?\",\"\\\\\",\"for \",\"true \",\"int \",\" abc \",\" \",\"/* comment */\",\"/\",\" abc123 \",\"a\",\n",
    "               \"123\",\"<unk>\"]\n",
    "\n",
    "d = dict((c, i) for i, c in enumerate(tokens_available))\n",
    "d_inv = dict((i, c) for i, c in enumerate(tokens_available))\n",
    "\n",
    "file = open('C:/Users/Thomas/Desktop/Data/CodRep_Sample/1.txt', \"r\", encoding = \"utf-8\")\n",
    "code = file.read()\n",
    "\n",
    "[tokens, lengths] = tokenize(code)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.539983279480944\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: Test Calc_Score\n",
    "with open(r\"C:\\CodeRepository\\Formatting-Error-Correction\\10-Gram Model\\\\10_gram_model_v2.p\", \"rb\") as fp:\n",
    "    lm = pickle.load(fp)\n",
    "# Tokens encoding\n",
    "tokens_enc = [d[x] for x in tokens]\n",
    "# Choose a snippet of 20 tokens\n",
    "# snip = tokens_enc = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \\\n",
    "#             + tokens_enc \\\n",
    "#             + [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "snip = tokens_enc[0:20]\n",
    "#print(snip)\n",
    "# Transform each token from its encoding to its real name\n",
    "snip_transf = []\n",
    "for i in snip:\n",
    "    snip_transf.append(tokens_available[i])\n",
    "#print(snip_transf)\n",
    "snip_padded = list(pad_both_ends(snip_transf,n=10))\n",
    "#print(snip_padded)\n",
    "snip_final = list(ngrams(snip_padded,10))\n",
    "score = lm.entropy(snip_final)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(text):\n",
    "    \"\"\"\n",
    "    Calculate the approximate cross-entropy of the n-gram model for a\n",
    "    given evaluation text.\n",
    "    This is the average log probability of each word in the text.\n",
    "\n",
    "    :param text: words to use for evaluation\n",
    "    :type text: list(str)\n",
    "    \"\"\"\n",
    "\n",
    "    e = 0.0\n",
    "    text = list(lpad) + text + list(rpad)\n",
    "    for i in range(n - 1, len(text)):\n",
    "        context = tuple(text[i - n + 1:i])\n",
    "        token = text[i]\n",
    "        e += logprob(token, context)\n",
    "    return e / float(len(text) - (n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5345f302c626f92c04e0f874461abf4c4b7dccbd8890a4c4a4ffb001b56aeba4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
