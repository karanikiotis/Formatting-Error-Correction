{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '/Users/Shared/c/CodeRepository/Formatting-Error-Correction/')\n",
    "import Scripts.S7_Parameters as Params\n",
    "from utils.tokenizer import tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(7)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(Params.tokensAvailable), 32, input_length = 20))\n",
    "model.add(tf.keras.layers.LSTM(300, input_shape = (20,32),return_sequences = True))\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.2))\n",
    "model.add(tf.keras.layers.LSTM(300, input_shape = (20,300)))\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.2))\n",
    "model.add(tf.keras.layers.Dense(41, activation = 'softmax', use_bias = True))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", min_delta = 0.025, mode =\"min\", patience = 3, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict((c, i) for i, c in enumerate(Params.tokensAvailable))\n",
    "d_inv = dict((i, c) for i, c in enumerate(Params.tokensAvailable))\n",
    "\n",
    "directory = '/Users/Shared/c/CodeRepository/Data/LSTM_TrainingDataset'\n",
    "os.chdir(directory)\n",
    "\n",
    "xData = []\n",
    "yData = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'\\n\\nStarting of corpus preprocessing:{start_time.strftime(\"%H:%M:%S\")}\\n\\n')\n",
    "for num, fileName in enumerate(sorted(os.listdir(directory))):\n",
    "    print(f'Iteration {num} -- Processing file with name: {fileName}...\\n')\n",
    "    file = open(fileName, \"r\", encoding = \"utf-8\", errors = 'ignore')\n",
    "    code = file.read()\n",
    "\n",
    "    [tokens, _] = tokenize(code)\n",
    "    \n",
    "    # Append <start> and <end> tokens   \n",
    "    tokens_enc = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \\\n",
    "                + [d[x] for x in tokens] \\\n",
    "                + [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens_enc)-19):\n",
    "        ngrams.append(tokens_enc[i : i+20])\n",
    "\n",
    "    # Calculation of index of the next token.\n",
    "    # On each position of the Numpy array, we have the index of the token\n",
    "    # that is going to appear after each 20-gram.\n",
    "    idxOfNextToken = []\n",
    "    for i in range(len(ngrams)-1):\n",
    "        idxOfNextToken.append(ngrams[i+1][19])\n",
    "    idxOfNextToken.append(1)\n",
    "\n",
    "    if(num == 0):\n",
    "        xData = ngrams\n",
    "        yData = idxOfNextToken\n",
    "    else:\n",
    "        xData += ngrams\n",
    "        yData += idxOfNextToken\n",
    "end_time = datetime.now()\n",
    "print(f'\\n\\nEnd of corpus preprocessing:{end_time.strftime(\"%H:%M:%S\")}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting the training of LSTM Network...\\n')\n",
    "xData = np.array(xData)\n",
    "yData = np.array(yData)\n",
    "yData = tf.keras.utils.to_categorical(yData, num_classes = len(Params.tokensAvailable))\n",
    "history = model.fit(xData, yData, epochs = 10, validation_split = 0.25, verbose = 1, batch_size = 128, callbacks = [earlystopping])\n",
    "\n",
    "model.save('/Users/Shared/c/CodeRepository/Formatting-Error-Correction/LSTM_Model/LSTM_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Shared/c/CodeRepository/Formatting-Error-Correction/LSTM_Model/history_LSTM_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (v3.8.10:3d8993a744, May  3 2021, 08:55:58) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
