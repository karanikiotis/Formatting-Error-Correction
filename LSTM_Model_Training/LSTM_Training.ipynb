{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '/Users/Shared/c/CodeRepository/Formatting-Error-Correction/')\n",
    "import Scripts.S7_Parameters as Params\n",
    "from utils.tokenizer import tokenize\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_style(\"whitegrid\",{'axes.grid' : False})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Training LSTM Network #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(7)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(Params.tokensAvailable), 32, input_length = 20))\n",
    "model.add(tf.keras.layers.LSTM(300, input_shape = (20,32),return_sequences = True))\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.2))\n",
    "model.add(tf.keras.layers.LSTM(300, input_shape = (20,300)))\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.2))\n",
    "model.add(tf.keras.layers.Dense(41, activation = 'softmax', use_bias = True))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", min_delta = 0.025, mode = \"min\", patience = 3, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict((c, i) for i, c in enumerate(Params.tokensAvailable))\n",
    "d_inv = dict((i, c) for i, c in enumerate(Params.tokensAvailable))\n",
    "directory = '/Users/Shared/c/CodeRepository/Data/LSTM_TrainingDataset'\n",
    "os.chdir(directory)\n",
    "xData = []\n",
    "yData = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'\\n\\nStarting of corpus preprocessing:{start_time.strftime(\"%H:%M:%S\")}\\n\\n')\n",
    "for num, fileName in enumerate(sorted(os.listdir(directory))):\n",
    "    print(f'Iteration {num} -- Processing file with name: {fileName}...\\n')\n",
    "    file = open(fileName, \"r\", encoding = \"utf-8\", errors = 'ignore')\n",
    "    code = file.read()\n",
    "\n",
    "    [tokens, _] = tokenize(code)\n",
    "    \n",
    "    # Append <start> and <end> tokens   \n",
    "    tokens_enc = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \\\n",
    "                + [d[x] for x in tokens] \\\n",
    "                + [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens_enc)-19):\n",
    "        ngrams.append(tokens_enc[i : i+20])\n",
    "\n",
    "    # Calculation of index of the next token.\n",
    "    # On each position of the Numpy array, we have the index of the token\n",
    "    # that is going to appear after each 20-gram.\n",
    "    idxOfNextToken = []\n",
    "    for i in range(len(ngrams)-1):\n",
    "        idxOfNextToken.append(ngrams[i+1][19])\n",
    "    idxOfNextToken.append(1)\n",
    "\n",
    "    if(num == 0):\n",
    "        xData = ngrams\n",
    "        yData = idxOfNextToken\n",
    "    else:\n",
    "        xData += ngrams\n",
    "        yData += idxOfNextToken\n",
    "end_time = datetime.now()\n",
    "print(f'\\n\\nEnd of corpus preprocessing:{end_time.strftime(\"%H:%M:%S\")}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting the training of LSTM Network...\\n')\n",
    "xData = np.array(xData)\n",
    "yData = np.array(yData)\n",
    "yData = tf.keras.utils.to_categorical(yData, num_classes = len(Params.tokensAvailable))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xData, yData, test_size = 0.2, random_state = 7)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 10, validation_data = (X_test, y_test), verbose = 1, batch_size = 128, callbacks = [earlystopping])\n",
    "\n",
    "#model.save('/Users/Shared/c/CodeRepository/Formatting-Error-Correction/LSTM_Model/LSTM_v4.h5')\n",
    "with open('/Users/Shared/c/CodeRepository/Formatting-Error-Correction/LSTM_Model/history_LSTM_v4.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LSTM Network Properties ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Shared/c/CodeRepository/Formatting-Error-Correction/LSTM_Model/history_LSTM_v4.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(1,len(history.history['accuracy'])+1)]\n",
    "plt.figure(figsize = [8,6])\n",
    "plt.plot(epochs,history.history['accuracy'])\n",
    "plt.plot(epochs,history.history['val_accuracy'])\n",
    "plt.title('LSTM - Training & Validation Categorical Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Categorical Accuracy')\n",
    "plt.yticks([0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87])\n",
    "plt.xticks(epochs)\n",
    "plt.legend(['Training', 'Validation'], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [8,6])\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('LSTM - Training & Validation Loss')\n",
    "plt.ylabel('Categorical Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.models.load_model(Params.path+\"LSTM_Model/LSTM_v4.h5\")\n",
    "lstm_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
