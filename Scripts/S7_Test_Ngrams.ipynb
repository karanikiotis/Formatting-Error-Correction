{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Functions - Classes used from NTLK Package\n",
    "\n",
    "#1. ngrams: used to make N-grams of a specified order\n",
    "\n",
    "#2. pad_both_ends: used to add start & end symbol to our corpus files-sentences. Order should be chosen according to n-grams order\n",
    "\n",
    "#3. everygrams: used to make n-grams of all orders. The highest order is defined through max_len parameter\n",
    "\n",
    "#4. flatten: Creates the vocab of our ngrams model. It consists of all words-tokens of our corpus\n",
    "\n",
    "#5. padded_everygram_pipeline: creates two iterators:\n",
    "        #i. sentences padded and turned into sequences \n",
    "        #ii. sentences padded as above and chained together for a flat stream of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'C:\\CodeRepository\\Formatting-Error-Correction')\n",
    "from datetime import datetime\n",
    "\n",
    "from Scripts.tokenizer import tokenize\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline,pad_both_ends\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from S1_corpus_bigrams_occurences import count_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,train_tok = count_occur('\\\\training_files')\n",
    "_,test_tok = count_occur('\\\\validation_files')\n",
    "\n",
    "order = [x for x in range(3,10)]\n",
    "\n",
    "entr = []\n",
    "j = 0\n",
    "\n",
    "for ord in order:\n",
    "    train_set,vocab = padded_everygram_pipeline(ord,train_tok)\n",
    "    kn = KneserNeyInterpolated(ord)\n",
    "    kn.fit(train_set,vocab)\n",
    "\n",
    "    current_score = []\n",
    "    for test in test_tok:\n",
    "        curr_test_padded = list(pad_both_ends(test,n=ord))\n",
    "        curr_test_set = list(everygrams(curr_test_padded,max_len = ord))\n",
    "\n",
    "        current_score.append(kn.entropy(curr_test_set))\n",
    "        \n",
    "    entr.append(sum(current_score)/len(current_score))\n",
    "    print(f'N-gram order:{ord}, Avg Score on Test Set:{entr[j]}, Time:{datetime.now()}')   \n",
    "\n",
    "    j+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord = 10\n",
    "train_set,vocab = padded_everygram_pipeline(ord,train_tok)\n",
    "kn = KneserNeyInterpolated(ord)\n",
    "kn.fit(train_set,vocab)\n",
    "\n",
    "current_score = []\n",
    "for test in test_tok:\n",
    "    curr_test_padded = list(pad_both_ends(test,n=ord))\n",
    "    curr_test_set = list(everygrams(curr_test_padded,max_len = ord))\n",
    "\n",
    "    current_score.append(kn.entropy(curr_test_set))\n",
    "        \n",
    "score = (sum(current_score)/len(current_score))\n",
    "print(f'N-gram order:{ord}, Avg Score on Test Set:{score}, Time:{datetime.now()}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on test1.txt file\n",
    "f = open(r'C:\\CodeRepository\\Formatting-Error-Correction\\Scripts\\test1.txt','r',encoding = 'utf-8')\n",
    "test_file = f.read()\n",
    "[test_tok, test_len] = tokenize(test_file)\n",
    "test_tok_padded = list(pad_both_ends(test_tok,n=ord))\n",
    "test_set = list(everygrams(test_tok_padded,max_len = ord))\n",
    "\n",
    "print(f'{kn.entropy(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.score_snippet import get_score\n",
    "f = open(r'C:\\CodeRepository\\Formatting-Error-Correction\\Scripts\\test1.txt','r',encoding = 'utf-8')\n",
    "test_file = f.read()\n",
    "[scores,_,_,_,_,_] = get_score(test_file)\n",
    "print(f'{scores}\\n')\n",
    "print(f'{sum(scores)/len(scores)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a48755110956b4e642d187f0ea4379f17d0ea5d04f44da7b7029377a566289d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
